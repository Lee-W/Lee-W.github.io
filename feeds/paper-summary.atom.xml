<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Life Lies in Traveling - Paper Summary</title><link href="http://lee-w.github.io/" rel="alternate"></link><link href="http://lee-w.github.io/feeds/paper-summary.atom.xml" rel="self"></link><id>http://lee-w.github.io/</id><updated>2017-04-05T22:30:00+08:00</updated><entry><title>[Paper Summary] Understanding Personality through Social Media</title><link href="http://lee-w.github.io/posts/paper-summary/2017/04/Understanding-Personality-through-Social-Media" rel="alternate"></link><published>2017-04-05T22:30:00+08:00</published><updated>2017-04-05T22:30:00+08:00</updated><author><name>Lee-W</name></author><id>tag:lee-w.github.io,2017-04-05:/posts/paper-summary/2017/04/Understanding-Personality-through-Social-Media</id><summary type="html">&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/1503/fc3acf17b1972c9a16e40b3eba6c2a140624.pdf"&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Main Purpose: To see how linguistic features correlate with each personality&amp;nbsp;trait.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/1503/fc3acf17b1972c9a16e40b3eba6c2a140624.pdf"&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Main Purpose: To see how linguistic features correlate with each personality&amp;nbsp;trait.&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;&lt;strong&gt;Use Twitter to predict &lt;span class="caps"&gt;MBIT&lt;/span&gt;&amp;nbsp;personality.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Problem of Past&amp;nbsp;Researches&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Language on social media has richer content that makes the typical linguistic analysis tool perform poorly (e.g. iono -&amp;gt; I don&amp;#8217;t&amp;nbsp;know)&lt;/li&gt;
&lt;li&gt;Gain personality information is costly (e.g. Big Five&amp;nbsp;Questionnaire)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;span class="caps"&gt;MBTI&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Instead of commonly used big five theory, &lt;span class="caps"&gt;MBTI&lt;/span&gt; is used in this&amp;nbsp;paper.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Myers–Briggs_Type_Indicator"&gt;Myers-Briggs Type&amp;nbsp;Indicator&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are 4 types of personality trait&lt;br&gt;&amp;nbsp;i.e.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introversion(I) /&amp;nbsp;Extraversion(E)&lt;/li&gt;
&lt;li&gt;Intuition(N) /&amp;nbsp;Sensing(S)&lt;/li&gt;
&lt;li&gt;Feeling(F) /&amp;nbsp;Thinking(T)&lt;/li&gt;
&lt;li&gt;Perception(P) /&amp;nbsp;Judging(J)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personality can be expressed as a code with 4 letters.&lt;br&gt;
e.g. &lt;span class="caps"&gt;ENFJ&lt;/span&gt;, &lt;span class="caps"&gt;INTP&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Twitter dataset&lt;ul&gt;
&lt;li&gt;Around 90,000&amp;nbsp;users&lt;/li&gt;
&lt;li&gt;120,000 personality-related tweets from 2006~2015 (out of 1.7 M&amp;nbsp;tweets)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;What is the so-called personliaty-related&amp;nbsp;tweets?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;English Tweets that contain users&amp;#8217; own &lt;span class="caps"&gt;MBIT&lt;/span&gt; code. &lt;br&gt;
    e.g.&lt;ul&gt;
&lt;li&gt;&lt;code&gt;"I'm an ENFJ"&lt;/code&gt; is&amp;nbsp;qualified&lt;/li&gt;
&lt;li&gt;&lt;code&gt;"My friend is an ISFJ"&lt;/code&gt; is not&amp;nbsp;qualified&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Heuristic rules is used (e.g. &lt;code&gt;"I'm"&lt;/code&gt;, &lt;code&gt;"I got"&lt;/code&gt;, &lt;code&gt;"I have been a"&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;No classification method is used for ensuring the personality code is indeed the&amp;nbsp;user&amp;#8217;s &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Distribution&lt;/h4&gt;
&lt;p&gt;Personality distribution of this data is&amp;nbsp;skewed.&lt;/p&gt;
&lt;p&gt;&lt;img alt="MBTI-bar" src="http://lee-w.github.io/images/posts-image/2017-04-05-understanding-personliaty-through-social-media/MBTI-bar.png"&gt;&lt;/p&gt;
&lt;p&gt;However, in the real word, the personality distribution might also be&amp;nbsp;skewed.&lt;/p&gt;
&lt;h3&gt;Features&lt;/h3&gt;
&lt;h4&gt;1.&amp;nbsp;n-grams&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Most frequent 1,000 unigram, bigram, trigram words and&amp;nbsp;phrases&lt;/li&gt;
&lt;li&gt;1,000 dimensions vectors for unigram, bigram trigram for each&amp;nbsp;user&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2. Twitter Part-of-speech&amp;nbsp;tags&lt;/h4&gt;
&lt;p&gt;Based on &lt;a href="http://www.cs.cmu.edu/~ark/TweetNLP/gimpel+etal.acl11.pdf"&gt;Part-of-Speech Tagging for Twitter: Annotation, Features, and&amp;nbsp;Experiments&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;25 types with some Twitter-specific tag. &lt;br&gt;
  e.g.&lt;ul&gt;
&lt;li&gt;hashtag&lt;/li&gt;
&lt;li&gt;at-mention&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;URL&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;emoticon &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3.word&amp;nbsp;vectors&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Word Vector&amp;nbsp;Settings&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2,334,564&amp;nbsp;words&lt;/li&gt;
&lt;li&gt;500&amp;nbsp;dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extracted&amp;nbsp;Features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average word&amp;nbsp;vectors&lt;/li&gt;
&lt;li&gt;Weighted average word vectors (weighted according to &lt;span class="caps"&gt;TF&lt;/span&gt;-&lt;span class="caps"&gt;IDF&lt;/span&gt;) &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;Logistic Regression is used (Random Forest and &lt;span class="caps"&gt;SVM&lt;/span&gt; produced similar&amp;nbsp;results)&lt;/p&gt;
&lt;h4&gt;Accuracy&amp;nbsp;Measurement&lt;/h4&gt;
&lt;p&gt;Since the data is skewed, &lt;span class="caps"&gt;AUC&lt;/span&gt; is&amp;nbsp;used.&lt;/p&gt;
&lt;h4&gt;Accuracy&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Indivisula Features&lt;ul&gt;
&lt;li&gt;Word Vector Only -&amp;gt; (&lt;span class="caps"&gt;AUC&lt;/span&gt;=0.651) &lt;/li&gt;
&lt;li&gt;n-gram only -&amp;gt; (&lt;span class="caps"&gt;AUC&lt;/span&gt;=0.607)&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;POS&lt;/span&gt; only -&amp;gt; (&lt;span class="caps"&gt;AUC&lt;/span&gt;=0.585)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Combinded Features&lt;ul&gt;
&lt;li&gt;All three features -&amp;gt; (&lt;span class="caps"&gt;AUC&lt;/span&gt;=0.661)&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;POS&lt;/span&gt; + n-gram -&amp;gt; (&lt;span class="caps"&gt;AUC&lt;/span&gt;=0.616)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Insight&lt;/h4&gt;
&lt;p&gt;Among the results, word vector performs best which might illustrate that predictions based on social media and language would&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;During the &lt;span class="caps"&gt;POS&lt;/span&gt; conversion process, information is compressed into 25 tags and might lost some important one.&lt;br&gt;
This might be the reason why it performs&amp;nbsp;worse.&lt;/p&gt;</content><category term="Machine Learning"></category><category term="NLP"></category><category term="MBTI"></category><category term="Personality"></category></entry></feed>